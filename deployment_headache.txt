---
  Updated Analysis - Now with Full Context

  After reading your code style docs (Tiger Style, Casey Muratori, Sean Goedecke),
   I can see much more clearly what's wrong with your deployment code and how to
  fix it properly.

  The Core Problems (Through Your Style Lens)

  1. MASSIVE VIOLATION: Casey's "Integration Discontinuities"

  Your deploy.py files are classic examples of integration discontinuities. Look
  at this progression:

  Initial integration: Just want to run training on GPU
    → Need custom bootstrap? Jump to managing uv PATH exports
    → Need different Python extra? Rewrite entire deploy.py
    → Need to sync different results? Rewrite sync_results()
    → Hit GPU memory issue? Massive rewrite for multi-GPU coordination

  Casey's point: You should always be able to take small steps. Right now, going
  from "run on 1 GPU" to "run on 8 GPUs with FSDP" requires understanding the
  entire deploy.py stack. That's a discontinuity.

  2. Casey's 5 Characteristics Applied to Your Code

  Let me evaluate your deployment code:

  Granularity: POOR ❌

  - deploy_code() bundles: SSH, push, bootstrap, PATH setup
  - Can't separate "just push code" from "install dependencies"
  - Can't just "check if GPU is available" without provisioning

  Casey would say: Break deploy_code() into:
  def push_code_only(bifrost: BifrostClient) -> str: ...
  def bootstrap_env(bifrost: BifrostClient, extra: str) -> None: ...
  def ensure_uv_installed(bifrost: BifrostClient) -> None: ...

  Redundancy: POOR ❌

  - Only ONE way to deploy: via full deploy.py orchestration
  - Can't just "sync results" without the whole deployment flow
  - No way to "just bootstrap" or "just start training"

  Casey would say: Provide multiple entry points:
  # High-level (coarse-grained)
  def deploy_and_run(config: Config) -> Results: ...

  # Mid-level
  def deploy_code(...) -> str: ...
  def start_training(...) -> str: ...
  def sync_results(...) -> None: ...

  # Low-level (fine-grained)  
  def push_files(...) -> str: ...
  def exec_bootstrap(...) -> None: ...
  def create_tmux_session(...) -> str: ...

  Coupling: TERRIBLE ❌❌❌

  The coupling is everywhere:

  # From integration_training/deploy.py:178
  bootstrap_cmd = 'export PATH="$HOME/.local/bin:$PATH" && uv sync --extra 
  dev-integration-training'
  workspace_path = bifrost_client.push(bootstrap_cmd=bootstrap_cmd)

  Coupled: code deployment + environment bootstrap + uv PATH setup + specific
  Python extra

  # deploy.py:203
  def start_training(...):
      venv_path = f"{workspace_path}/.venv/bin/activate"  # Coupled to uv 
  structure
      project_dir = f"{workspace_path}/dev/integration_training"  # Coupled to 
  monorepo layout
      # ... 50 more lines of coupled logic

  Casey would say: "Hey, I may want some control over what was getting done here
  right? I may not want it to just use the uv venv directly. I may want to play
  with it."

  Retention: MODERATE ⚠️

  - Config objects retain a lot: deployment.gpu_ranks, deployment.min_vram, etc.
  - But at least you're not forcing me to register callbacks or build scene graphs

  Flow Control: GOOD ✅

  - Mostly straightforward: my code calls bifrost, bifrost returns
  - No deep callback hell (except tmux polling loops)

  3. Tiger Style Violations

  Your deploy.py files are 400-600 lines each. Tiger Style: 70 lines max per 
  function.

  # integration_training/deploy.py has:
  def main(): 565 lines  # 8x over limit ❌
  def start_training(): 93 lines  # Over limit ❌
  def wait_for_completion(): 55 lines  # OK ✅

  Tiger Style principle: "Split code into functions. There are many ways to cut a
  wall of code into chunks of 70 lines, but only a few splits will feel right."

  Your main() should be:
  def main():
      config = load_config(args.config)
      gpu = provision_gpu_if_needed(gpu_client, config)
      workspace = deploy_code_to_gpu(bifrost, config)
      session = start_remote_training(bifrost, config, workspace)

      if not args.detached:
          success = wait_for_completion(bifrost, session)
          sync_results_to_local(bifrost, config)

      cleanup_if_requested(gpu, config)

  4. Sean Goedecke: "Good design looks like nothing"

  From Sean's doc:
  "Good system design looks underwhelming. In practice, it looks like nothing 
  going wrong for a long time. You can tell that you're in the presence of good 
  design if you have thoughts like 'huh, this ended up being easier than I 
  expected'."

  Your deployment code is the opposite of this. You said yourself: "I am at my
  wits end and this is typically the flakiest part of setting up a new project."

  That's the signal that the design is wrong.

  5. The UV.LOCK Problem is State Management (Sean Goedecke)

  From Sean:
  "The hard part about software design is state. You should try and minimize the 
  amount of stateful components in any system."

  Your uv.lock is massive state: 1MB, workspace-wide, with every possible
  dependency. When you deploy, you're syncing this giant state blob to remote
  machines that only need 10% of it.

  Sean's solution: "One service that knows about the state - i.e. it talks to a
  database - and other services that do stateless things."

  Applied to your code: One workspace root with the lock file, but each dev/
  project should be stateless - it just declares its deps in pyproject.toml, and
  uv resolves them from the workspace lock.

  But you're doing the opposite: trying to make deploy.py manage the state.

  ---
  The Casey Muratori Solution: API Design for Deployment

  Casey's whole talk is about: How do I design reusable APIs that don't force 
  integration discontinuities?

  Your problem: Deployment is not reusable across projects because each deploy.py
  reimplements everything.

  Casey's checklist applied:

  ✅ Write the usage code first

  # What SHOULD deployment look like? Let's write it:

  # Simple case (speedrun):
  deploy.run("configs/baseline.py", extra="dev-speedrun")

  # Custom case (integration_training):  
  gpu = deploy.provision(Config.from_file("configs/01_debug.py"))
  ws = deploy.push_code(gpu, extra="dev-integration-training")
  session = deploy.start_tmux(gpu, ws, cmd="python train.py configs/01_debug.py")
  deploy.tail_logs(gpu, session)

  # Just sync results (no deployment):
  deploy.sync("root@host:22", remote_dir="results/exp_123", local_dir="./results")

  That's what the API should be. Now work backwards.

  ✅ Any retain-mode construct should have an immediate-mode equivalent

  Current (bad): Deploy.py orchestrates everything, holds all the state
  # I HAVE to go through the full main() orchestration
  python deploy.py configs/01.py --ssh root@host:22

  Casey's version (good):
  # Retain-mode (high-level, convenient):
  deploy.run_full_pipeline(config, ssh_connection)

  # Immediate-mode (low-level, flexible):
  deploy.provision(...)
  deploy.push(...)
  deploy.start(...)
  deploy.wait(...)
  deploy.sync(...)

  ✅ Never require callbacks or inheritance

  Your code is good here! No forced callbacks. But you DO force a specific
  workflow (the full deploy.py script).

  ✅ No required specific data types

  Violation: You force Config objects with specific nested structure:
  config.deployment.gpu_ranks  # Must be exactly this structure
  config.output.save_dir

  Casey's version:
  # Allow native Python types:
  deploy.provision(gpu_count=8, min_vram=80, max_price=10.0)  # Just kwargs

  # But ALSO allow config objects for convenience:
  deploy.provision(config.deployment)  # Pass whole struct if you want

  ✅ Data should be transparent, not opaque

  Good! Your Config uses dataclasses, which are transparent. Not hiding anything
  behind getters/setters.

  But the remote workspace structure is opaque:
  workspace_path = bifrost_client.push(...)  # Returns "~/.bifrost/workspace"
  # Then I have to KNOW it creates: ~/.bifrost/workspace/dev/integration_training/
  # That knowledge is hidden inside bifrost, coupled to monorepo layout

  Casey's version: Make the layout explicit and configurable:
  workspace = bifrost.push(
      local_path="dev/integration_training",
      remote_path="~/my-custom-path"  # I control where it goes
  )

  ---
  Concrete Recommendations (Casey + Tiger + Sean Style)

  1. CREATE shared/deploy_api.py (70 lines per function max)

  Following Tiger Style, Casey's API design, and Sean's "boring is good":

  """Deployment API - boring, well-tested components.

  Casey's principles:
  - Granular: Break big operations into small ones
  - Redundant: Provide both high-level and low-level APIs  
  - Decoupled: Each function does ONE thing
  - Immediate mode: No hidden state, just function calls

  Tiger Style:
  - Max 70 lines per function
  - Explicit control flow
  - Assert preconditions

  Sean Goedecke:
  - Minimize state
  - Should look underwhelming
  """

  from dataclasses import dataclass
  from bifrost import BifrostClient
  from broker import GPUClient, GPUInstance

  # === TYPES (transparent data, not opaque objects) ===

  @dataclass
  class GPUSpec:
      """GPU requirements - plain data, no methods."""
      count: int = 1
      min_vram_gb: int | None = None
      max_price_usd: float = 10.0

  @dataclass  
  class Workspace:
      """Remote workspace info - plain data."""
      path: str
      venv_path: str

  # === GRANULAR OPERATIONS (fine-grained) ===

  def provision_gpu(
      client: GPUClient,
      spec: GPUSpec,
  ) -> GPUInstance:
      """Provision a single GPU. Does ONE thing."""
      # ... implementation (< 70 lines)

  def push_code(
      bifrost: BifrostClient,
      local_path: str = ".",
      remote_path: str = "~/.bifrost/workspace",
  ) -> str:
      """Push code to remote. No bootstrap, just files.
      
      Returns: Absolute remote path
      """
      # ... implementation (< 70 lines)

  def bootstrap_python(
      bifrost: BifrostClient,
      workspace: str,
      extra: str,
  ) -> None:
      """Install Python dependencies. Separate from code push.
      
      Handles: uv installation, PATH setup, uv sync
      """
      # ... implementation (< 70 lines)

  def start_tmux(
      bifrost: BifrostClient,
      session_name: str,
      command: str,
  ) -> None:
      """Start a tmux session with command. Just tmux, nothing else."""
      # ... implementation (< 70 lines)

  def sync_files(
      bifrost: BifrostClient,
      remote_path: str,
      local_path: str,
  ) -> None:
      """Download files from remote. Just sync, nothing else."""
      # ... implementation (< 70 lines)

  # === REDUNDANT HIGH-LEVEL API (coarse-grained) ===

  def deploy_and_run(
      config: BaseConfig,  # Your standard config
      ssh_connection: str,
      detached: bool = False,
  ) -> None:
      """Full deployment orchestration. Calls all the granular ops.
      
      This is the "update orientation" from Casey's talk - 
      bundles everything for convenience.
      """
      # Calls: provision_gpu, push_code, bootstrap_python, 
      #        start_tmux, wait_for_completion, sync_files
      # ... implementation (< 70 lines - mostly just calling other functions)

  Key insight from Casey: Users can call deploy_and_run() for convenience OR call
  the granular functions when they need control. No discontinuities.

  2. FIX UV WORKSPACE (Sean's stateless principle)

  Problem: 1MB monorepo lock file with all deps for all projects

  Solution A: Per-project lock files
  # Each dev/X project is its own uv workspace member with own lock
  research/
    pyproject.toml           # Workspace declaration only
    dev/
      integration_training/
        pyproject.toml        # Project deps
        uv.lock               # Just THIS project's lock (small)
      speedrun/
        pyproject.toml
        uv.lock

  Then bifrost.push() only syncs the project subdir, not the whole monorepo.

  Solution B: Make projects stateless, use shared lock smarter
  # In deploy_api.py:
  def bootstrap_python(bifrost: BifrostClient, workspace: str, extra: str):
      """Bootstrap ONLY the deps for this extra group."""
      # Use uv's --only-group or --package to install just this project
      cmd = f"uv sync --only-group {extra}"  # If uv supports this
      bifrost.exec(cmd, working_dir=workspace)

  This keeps projects stateless (no per-project lock) but avoids syncing unused
  deps.

  3. HIERARCHICAL CONFIG (from experiment_config.md)

  You already documented this! Just implement it:

  # shared/config.py
  @dataclass
  class DeploymentConfig:
      """Shared deployment config (all projects use this)."""
      gpu_count: int = 1
      min_vram: int | None = None
      max_price: float = 10.0

  @dataclass
  class OutputConfig:
      """Shared output config."""
      experiment_name: str
      save_dir: Path
      log_level: str = "INFO"

  @dataclass
  class BaseConfig:
      """All projects inherit from this."""
      deployment: DeploymentConfig = field(default_factory=DeploymentConfig)
      output: OutputConfig = field(default_factory=OutputConfig)

  # Then in each project:
  # dev/integration_training/config.py
  @dataclass
  class IntegrationTrainingConfig(BaseConfig):
      model: ModelConfig = field(default_factory=ModelConfig)
      training: TrainingConfig = field(default_factory=TrainingConfig)

  Result: No more duplicating deployment/output config across 4 projects!

  4. REFACTOR DEPLOY.PY (Tiger Style: 70 lines per function)

  # dev/integration_training/deploy.py (NEW VERSION)
  """Deployment for integration training.

  Now just a thin wrapper over shared/deploy_api.py.
  """

  from shared.deploy_api import *
  from shared.config import BaseConfig
  from .config import IntegrationTrainingConfig

  def main():
      """Main entry point - should be ~50 lines."""
      parser = create_parser()
      args = parser.parse_args()

      config = IntegrationTrainingConfig.from_file(args.config)

      if args.use_existing:
          # Just connect, don't provision
          bifrost = connect_existing(args.use_existing, args.ssh_key)
      else:
          # Provision new GPU
          gpu = provision_gpu(gpu_client, config.deployment)
          bifrost = BifrostClient(gpu.ssh_connection_string(), args.ssh_key)

      # Deploy
      workspace = deploy_project(
          bifrost=bifrost,
          project_path="dev/integration_training",
          extra="dev-integration-training"
      )

      # Start training
      session = start_training_session(
          bifrost=bifrost,
          workspace=workspace,
          config=config,
      )

      # Wait or detach
      if args.detached:
          print(f"Training started: tmux attach -t {session}")
      else:
          wait_and_sync(bifrost, session, config)

      cleanup_if_requested(gpu, config)

  # Each helper function < 70 lines, calls shared/deploy_api.py

  Result: Deploy.py goes from 600 lines → 150 lines, mostly config-specific logic.

  ---
  How This Fixes Your Pain

  "I am at my wits end and this is typically the flakiest part of setting up a new
   project."

  Before (current):
  - New project → copy 400 lines of deploy.py → modify for new paths/extras →
  debug bootstrap issues → repeat 4 times

  After (proposed):
  - New project → write 100-line deploy.py that calls shared/deploy_api → done

  Before (debugging):
  - Something breaks → grep through 600 lines of deploy.py → find bug in line 347
  → realize 3 other deploy.py files have same bug → fix all 4

  After (proposed):
  - Something breaks → grep finds bug in shared/deploy_api.py → fix once → all
  projects fixed

  Before (changing workflow):
  - Want to "just sync results without deploying" → either hack deploy.py or run
  bifrost commands manually → no reusable solution

  After (proposed):
  - Want to sync results → deploy.sync_files(bifrost, remote="results/exp", 
  local="./results") → done

  ---
  Next Steps - What Should I Build?

  I can help you implement this. Here's the order I recommend:

  Phase 1: Extract common code (1-2 days)
  1. Create shared/deploy_api.py with granular functions
  2. Extract: provision_gpu(), push_code(), bootstrap_python(), start_tmux(),
  sync_files()
  3. Keep each function < 70 lines (Tiger Style)

  Phase 2: Shared config (1 day)
  1. Create shared/config.py with BaseConfig
  2. Move DeploymentConfig and OutputConfig there
  3. Update each project's config to inherit from BaseConfig

  Phase 3: Refactor deploy.py files (1-2 days)
  1. Rewrite each deploy.py to use shared/deploy_api
  2. Each deploy.py → ~150 lines (was 400-600)
  3. Mostly project-specific logic, not boilerplate

  Phase 4: Fix uv workspace (2-3 days)
  1. Either split into per-project locks OR
  2. Use uv's --package/--only-group features
  3. Test that deploys only sync needed deps

  Which phase do you want me to start with? I'm ready to write code whenever you
  give the word.

